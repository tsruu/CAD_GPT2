{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoConfig, AutoTokenizer, GPT2LMHeadModel\n",
        "from transformers.modeling_outputs import CausalLMOutputWithPast"
      ],
      "metadata": {
        "id": "64lKdrvefu0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CADGPT2LMHeadModel(GPT2LMHeadModel):\n",
        "    def __init__(self, config, alpha=0.5, max_length=1024):\n",
        "        super().__init__(config)\n",
        "        self.alpha = alpha\n",
        "        self.max_length = max_length\n",
        "        self.context_ids = None\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config._name_or_path)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_name, alpha=0.5, *model_args, **kwargs):\n",
        "\n",
        "        config = AutoConfig.from_pretrained(model_name)\n",
        "        model = super().from_pretrained(model_name, *model_args, config=config, **kwargs)\n",
        "        model.alpha = alpha\n",
        "        return model\n",
        "\n",
        "    def set_context(self, context_text):\n",
        "\n",
        "        if context_text:\n",
        "            self.context_ids = self.tokenizer(context_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "            self.context_ids = self.context_ids.to(next(self.parameters()).device)\n",
        "        else:\n",
        "            self.context_ids = None\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, use_context_aware=True, **kwargs):\n",
        "        if past_key_values is not None:\n",
        "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
        "        else:\n",
        "            if use_context_aware and self.context_ids is not None:\n",
        "\n",
        "                batch_size = input_ids.size(0)\n",
        "                context_ids_expanded = self.context_ids.expand(batch_size, -1)\n",
        "                input_ids = torch.cat([context_ids_expanded, input_ids], dim=1)\n",
        "\n",
        "                attention_mask = kwargs.get('attention_mask', None)\n",
        "                if attention_mask is not None:\n",
        "                    attention_mask = torch.cat(\n",
        "                        [torch.ones(batch_size, self.context_ids.size(1), device=input_ids.device), attention_mask], dim=1\n",
        "                    )\n",
        "                    kwargs['attention_mask'] = attention_mask\n",
        "\n",
        "                seq_length = input_ids.size(1)\n",
        "                position_ids = torch.arange(0, seq_length, dtype=torch.long, device=input_ids.device)\n",
        "                position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
        "                kwargs['position_ids'] = position_ids\n",
        "\n",
        "\n",
        "                self.context_ids = None\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"past_key_values\": past_key_values,\n",
        "            \"use_cache\": kwargs.get(\"use_cache\"),\n",
        "            \"attention_mask\": kwargs.get(\"attention_mask\", None),\n",
        "            \"position_ids\": kwargs.get(\"position_ids\", None),\n",
        "            \"token_type_ids\": kwargs.get(\"token_type_ids\", None),\n",
        "        }\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, position_ids=None, past_key_values=None, use_context_aware=True, **kwargs):\n",
        "        if past_key_values is not None:\n",
        "\n",
        "            return super().forward(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                position_ids=position_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                **kwargs\n",
        "            )\n",
        "        else:\n",
        "\n",
        "            outputs_without_context = super().forward(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                position_ids=position_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                **kwargs\n",
        "            )\n",
        "            logits_without_context = outputs_without_context.logits\n",
        "\n",
        "\n",
        "            if use_context_aware and self.context_ids is not None:\n",
        "                batch_size = input_ids.size(0)\n",
        "\n",
        "                context_ids_expanded = self.context_ids.expand(batch_size, -1)\n",
        "                input_ids_with_context = torch.cat([context_ids_expanded, input_ids], dim=1)\n",
        "\n",
        "                if attention_mask is not None:\n",
        "                    attention_mask_with_context = torch.cat(\n",
        "                        [torch.ones(batch_size, self.context_ids.size(1), device=input_ids.device), attention_mask], dim=1\n",
        "                    )\n",
        "                else:\n",
        "                    attention_mask_with_context = None\n",
        "\n",
        "                seq_length = input_ids_with_context.size(1)\n",
        "                position_ids = torch.arange(0, seq_length, dtype=torch.long, device=input_ids.device)\n",
        "                position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "                transformer_outputs = self.transformer(\n",
        "                    input_ids=input_ids_with_context,\n",
        "                    attention_mask=attention_mask_with_context,\n",
        "                    position_ids=position_ids,\n",
        "                    past_key_values=None,\n",
        "                    **kwargs\n",
        "                )\n",
        "\n",
        "                logits_with_context = transformer_outputs.last_hidden_state[:, -input_ids.size(1):, :]\n",
        "                lm_logits = self.lm_head(logits_with_context)\n",
        "\n",
        "                adjusted_logits = (1 + self.alpha) * lm_logits - self.alpha * logits_without_context\n",
        "\n",
        "                return CausalLMOutputWithPast(\n",
        "                    logits=adjusted_logits,\n",
        "                    past_key_values=transformer_outputs.past_key_values,\n",
        "                    hidden_states=transformer_outputs.hidden_states,\n",
        "                    attentions=transformer_outputs.attentions\n",
        "                )\n",
        "\n",
        "            return outputs_without_context"
      ],
      "metadata": {
        "id": "arnEgqP6f0Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "alpha = 0.5\n",
        "model = CADGPT2LMHeadModel.from_pretrained(model_name, alpha=alpha)"
      ],
      "metadata": {
        "id": "CHBfxmLPf529"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_text = \"Argentina won the World Cup in 1345, 1978, 1986, and 2022.\"\n",
        "model.set_context(context_text)\n",
        "\n",
        "input_text = \"How many times has Argentina won the World Cup?\"\n",
        "input_ids = model.tokenizer(input_text, return_tensors=\"pt\").input_ids.to(next(model.parameters()).device)\n",
        "\n",
        "output_ids = model.generate(input_ids=input_ids, max_length=80, do_sample=True)\n",
        "output_text = model.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msfyXBfigS1E",
        "outputId": "20b566a0-6474-43c0-ca2c-14caa8b58105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How many times has Argentina won the World Cup?\n",
            "\n",
            "Argentina won the World Cup in 1345, 1978, 1986, and 2022. How many times has Argentina won the World Cup?\n",
            "\n",
            "Q: Is the World Cup about creativity or competition?\n",
            "\n",
            "A: We don't think so. Every tournament has different goals and the same goals can be scored at different times.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}